---
title: "Assessing assemblies quality"
teaching: 15
exercises: 25
questions:
- "Which assembly contains longer contigs?"
- "Which assembly best represents the raw data (sequencing reads)?"
objectives:
- "Assess the quality of your assemblies"
keypoints:
- "checkV assesses the quality of your contigs with regard to viral completeness and contamination"
- "minimap2 aligns long and noisy nanopore reads efficiently to large (meta)genomes"
- "samtools can be used to read, filter, convert and summarize alignments"
---

# Assemblies assessment

Now we will measure some basic aspects of the assemblies, such as the fragmentation degree and the percentage of the raw data they contain. Ideally, the assembly would contain a single and complete contig for each species in the sample, and would represent 100% of the sequencing reads.

### Fragmentation

Use [checkV](https://bitbucket.org/berkeleylab/checkv/src/master/) to assess the completeness and contamination of the assemblies...

~~~
# create a folder for the assessment (or let sbatch create it when you assign the output and error log files)
$ mkdir 30_results_assessment_checkv

# activate the conda environment containing the checkv installation
$ source /vast/groups/VEO/tools/anaconda3/etc/profile.d/conda.sh && conda activate checkv_v1.0.1

# run checkV on both assemblies
$ checkv end_to_end ...
~~~
{: .language-bash}

> ## sbatch script for running checkV
> ```bash
> #!/bin/bash
> #SBATCH --tasks=1
> #SBATCH --cpus-per-task=22
> #SBATCH --partition=standard
> #SBATCH --mem=20G
> #SBATCH --time=02:30:00
> #SBATCH --job-name=assessment_checkv
> #SBATCH --output=30_results_assessment_checkv/assessment_checkv.slurm.%j.out
> #SBATCH --error=30_results_assessment_checkv/assessment_checkv.slurm.%j.err
> 
> # run CheckV to assess the completeness of single-contig virus genomes.
> # First, activate the conda environment which holds the CheckV installation on draco:
> source /vast/groups/VEO/tools/anaconda3/etc/profile.d/conda.sh && conda activate checkv_v1.0.1
>
> # CheckV parameters (https://bitbucket.org/berkeleylab/checkv/src/master/#markdown-header-running-checkv)
> # checkv end-to-end runs the CheckV pipeline from end to end :). It expects an input fasta file 
> # with the assembly and an output path.
> # -t: threads
> checkv end_to_end -t 20 -d /work/groups/VEO/databases/checkv/v1.5 10_results_assembly_flye/cross_assembly/assembly.fasta 30_results_assessment_checkv/cross_assembly
> checkv end_to_end -t 20 -d /work/groups/VEO/databases/checkv/v1.5 10_results_assembly_flye/single_assemblies/merged_single_assemblies.fasta 30_results_assessment_checkv/single_assemblies
> 
> # technically its not necessary to close the conda environment, 
> # the session will be terminated after the script finishes.
> conda deactivate
> ```
> {: .source}
{: .solution}

### Raw data representation

Use `minimap2` to align the sequences of each sample to the assemblies and save the results in a SAM format (ie. `barcode62.sam`, `barcode64.sam` and `barcode64.sam` once for each assembly). Then use `samtools view` to convert the SAM files to BAM format, which is the binary form of the SAM format. Once you have the BAM files, sort them with `samtools sort`. Last, get basic stats of the mapping using `samtools stats`. You can also pipe the respective outputs of each step into the next step, saving disc IO and possibly speeding up things (minimap2 and samtools are specifically designed for this).

~~~
# index the assemblies
$ bwa index <ASSEMBLY_FASTA>

# map the reads to each assembly
$ minimap2 ... > 1_assemblies/assessment/<OUTPUT_SAM>

# convert SAM file to BAM file
$ samtools view ...

# sort the BAM file
$ samtools sort ...

# get mapping statistics
$ samtools stats ...
~~~
{: .language-bash}

> ## sbatch script for aligning the samples to the assemblies
> ```bash
> #!/bin/bash
> #SBATCH --tasks=1
> #SBATCH --cpus-per-task=32
> #SBATCH --partition=standard
> #SBATCH --mem=20G
> #SBATCH --time=00:30:00
> #SBATCH --job-name=alignment_minimap2
> #SBATCH --output=30_results_alignment_minimap2/alignment_minimap2.slurm.%j.out
> #SBATCH --error=30_results_alignment_minimap2/alignment_minimap2.slurm.%j.err
> 
> # assign tool paths to aliases for better readability
> minimap2='/home/groups/VEO/tools/minimap2/v2.26/minimap2'
> samtools='/home/groups/VEO/tools/samtools/v1.17/bin/samtools'
>
> indir='../data/sequences'
> outdir='../data/alignments/cross_assembly'
> mkdir -p $outdir
> 
> # run minimap2 to align all reads from a sample to the assembled contigs
> # and pipe the output into samtools for conversion into the binary bam format
> #
> # minimap2 parameters (https://lh3.github.io/minimap2/minimap2.html):
> # -x map_ont : Use a preset for parameterizing the affine gap penalty model for the extension of matched seeds
> # suited for noisy nanopore reads.
> # -a : output in SAM format
> # -t 30 : run with 30 threads
> #
> # samtools parameters (http://www.htslib.org/doc/samtools.html):
> # samtools view can be used to convert between SAM, BAM and CRAM formats.
> # view -u : output uncompressed binary format (BAM)
> # samtools sort can be used to sort a SAM, BAM or CRAM file. Some tools expect sorted alignments.
> # sort --write-index : output the index of the sorted alignments, can reduce file IO when accessing only a subset of the alignments
> # sort -o : set the output file for the sorted alignments
> #
> # - : the - tells samtools to take the inpute from the pipe (| is the piping operator).
> for barcode in $(seq 62 64) 
> do 
>   $minimap2 -x map-ont -a -t 30  10_results_assembly_flye/cross_assembly/assembly.fasta $indir/barcode$barcode.fastq.gz | \
>     $samtools view -u - | $samtools sort -o $outdir/barcode$barcode.bam --write-index -
>   $samtools stats $outdir/barcode$barcode.bam > $outdir/barcode$barcode_stats.txt
> done
>
> outdir='../data/alignments/single_assemblies/'
> mkdir -p $outdir
> for barcode in $(seq 62 64) 
> do 
>   $minimap2 -x map-ont -a -t 30  10_results_assembly_flye/single_assemblies/merged_single_assemblies.fasta $indir/barcode$barcode.fastq.gz | \
>     $samtools view -u - | $samtools sort -o $outdir/barcode$barcode.bam --write-index -
>   $samtools stats $outdir/barcode$barcode.bam > $outdir/barcode$barcode_stats.txt
> done
> ```
> {: .source}
{: .solution}

> ## Compare both assemblies
> So far you have calculated some metrics to assess the quality of the assemblies, but bare in mind there also exist also others we can check for this, such as the number of ORFs or the depth of coverage across the contigs.
> In the report generated by Quast, look at metrics regarding scaffolds length, such as the N50. Can you explain the difference between both assemblies? Regarding the raw data containment, how different are both assemblies? Which metric do you find more relevant for metagenomics? Can you think of other metric to assess the quality of an assembly?
{: .discussion}




{% include links.md %}
