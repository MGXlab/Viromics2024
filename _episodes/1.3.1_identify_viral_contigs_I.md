---
start: True
title: "Identifying Viral Contigs I"
exercises: 180
teaching: 0
questions:
objectives:
- "Understand the concept and structure of benchmarking virus identification tools"
- "Evaluate the benchmarking results presented in the paper by Wu *et al*"
- "Choose a virus identification tool"
keypoints:
- "Different approaches can be used in a virus identification tool, such as reference-based and machine learning"
- "Benchmark is a comparison between tools and should offer concrete evidence of performance, such as number of TP, FN, FP and TN"
- "When choosing a tool, you should consider the priorities of the project. E. g. you need to identify as many viruses as possible and FP are not critical, so a tool with high recall is ideal"
---

# Comparing Virus Identification Tools

In this theoretical part, you will dive into a very important part of microbiome data analysis: how to choose a good tool. Several tools for identifying 
viruses are available online. Each measure different biological signals and use different algorithms and databases. To choose the best one for your project,
you should consider what you want to achieve in your research and the overall performance of the tools (such as sensitivity and specificity). If a recent tool does the same task as previously published ones, the authors should compare (or benchmark) them. Ideally, independent authors compare different tools, which is a great guide for scientists to choose a good tool.   

Today, you will work on the paper: ["Benchmarking bioinformatic virus identification tools using realâ€world metagenomic data across biomes"](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-024-03236-4), by Wu *et al* 2024. Considering the allocated time for this activity, you will probably not be able to read the entire paper. Therefore, focus on Figures 1, 3 and 6 (and search online whenever necessary) to answer the proposed questions. 

> ## challenge
>
> Authors investigated nine tools, which can be divided into three groups. What are the groups' descriptions and how do they differ?
>   
> What are the True Positives, False Negatives, False Positives and True Negatives in Fig 1? Why are these important for the benchmarking?
> 
> In Fig 3 A,B,C, what indicates a good performace?
>   
> In Fig 3 D,E,F, they used TPR, FPR and AUC/ROC for evaluation. What are those terms? What is the main takeaway from these figures?
> 
> Overall, which tools in Fig 3 are better? Why do you think that is?
> 
> What is the description of Fig 6? What value does this analysis add to the benchmarking?
> 
> Taking all you have learned from the paper into consideration, is there an ideal tool to indentify viruses and why?
> 
> For the project you are developing in this module, which of these tools would you choose and why?
> 
> {: .source}
{: .discussion}

# Supplementary reference

The [wiki about precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) is a good source to understand measurements that take into account the balance between TP, FN and FP.

