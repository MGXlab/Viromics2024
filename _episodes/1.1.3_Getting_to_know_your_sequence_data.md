---
title: Getting to know your sequence data
teaching: 120
exercises: 0
questions: []
objectives:
- Interpret different file formats used in bioinformatics
- Utilize bash commands to look into sequence data
- Get a basic understanding of what sequence data looks like
- Work with sequence data in Python or R to create basic plots
keypoints:
---
## Navigating your filesystem space

You can use the following to get into Draco

```bash
ssh <fsuid>@login1.draco.uni-jena.de

# OR

ssh <fsuid>@login2.draco.uni-jena.de
```

Take a look at the file structure and files. We have given you a subset of viromics data. Some helpful commands might be

```bash
# list
ls
ll

# path to working directory
pwd

# print first 10 lines
head -10
# print last 10 lines
tail -10

# open file on terminal (press 'q' to exit less)
less
more

# count number of lines
wc -l file.txt

# search for "@" inside file 
grep "@" file.fastq
```

[See more useful commands and one-liners here](https://github.com/vmkhot/useful-scripts/blob/main/Linux%20Commands%20Cheat%20Sheet.md#linux-commands-cheat-sheet)

## Understanding bioinformatics file formats

Learn about fastq, fasta, sam/bam/bai, genbank, gff here

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/KZ2wqKFerG0/0.jpg)](https://www.youtube.com/watch?v=KZ2wqKFerG0&ab_channel=edu-ome)

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/D4WDdAbZW1Y/0.jpg)]( https://www.youtube.com/watch?v=D4WDdAbZW1Y&ab_channel=BasE.Dutilh)


## Submitting jobs on Draco

When you login to Draco, you are on the "login node" and this is not the best place to run any programs or heavier scripts. To do this, you have to be on a "compute node".

You can either request for resources from a compute node and run programs interactively or submit a job to the job scheduler, which then sends it to a compute node to complete.

![slurm_architecture_simple](https://github.com/user-attachments/assets/f6baeec5-ddf0-413d-909c-aa2d2b947bd5)


To get resources for an interactive session on the short node for 1 hour with 10 cpus and 1G memory - maybe for testing a program or script

```bash
salloc --partition=short --time=01:00:00 --cpus-per-task=10 --mem=1G
```

To submit a job, you have to make a script `my_slurm_script.sh` with follow headers. (Change the parameters) [example here](https://github.com/vmkhot/useful-scripts/blob/main/example_sbatch.md)

```bash

#!/bin/bash
#SBATCH --tasks=1
#SBATCH --cpus-per-task=<threads>
#SBATCH --partition=<partitions,listed,here>
#SBATCH --mem=<>
#SBATCH --time=<hh:mm:ss>
#SBATCH --job-name=<job_name_id>
#SBATCH --output=<outdir>/<tool>.slurm.out.%j
#SBATCH --error=<outdir>/<tool>.slurm.err.%j

# activate the conda environment

source /path/to/miniconda3/etc/profile.d/conda.sh && conda activate <tool_env>

echo date

my commands here

```

To submit the script

```bash
sbatch my_slurm_script
```

Other useful slurm commands

```bash
# submit a job
sbatch

# check on your jobs
squeue -u <fsuid>

# how many resources are being consumed
sstat <job id>

# kill a job
scancel <job id>

# information about nodes
sinfo --long

scontrol show node
```

> ## Exercise
>
> Describe fastq and fasta file formats
> 
> - What information is contained in sequencing files?
>     - Choose 2 (other than fasta) and describe
> - How many lines in your fastq files?
>     - How many sequences?
> - Print the first 10 and last 10 in terminal
>     - Describe these lines
>     - What's the difference?
> - What is the GC content of your files?
>     - plot frequency v. GC content
>     - describe the distribution
>  
>  Some libraries you might use are: 
>        Python: seaborn: distplot, matplotlib: hist
>        R: ggplot: geom_hist
> 
> {: .source}
{: .challenge}
